# Memory Block Consolidation Process

This document outlines the process by which the Hindsight AI memory service consolidates similar or duplicate memory blocks into single, refined suggestions. The consolidation is performed by a background worker (`core.workers.consolidation_worker`) that leverages either a Large Language Model (LLM) or a fallback similarity algorithm.

## 1. Overview

The consolidation worker's primary goal is to identify redundant information across multiple memory blocks and propose a single, comprehensive memory block that captures the essence of the original set. This helps in maintaining a clean, efficient, and high-quality memory store for the AI agent.

## 2. Process Flow

The `run_consolidation_analysis` function orchestrates the entire process:

1.  **Fetch Memory Blocks:** Memory blocks are retrieved from the database in batches using the `fetch_memory_blocks` function. This ensures efficient processing, especially for large datasets.
2.  **Analyze Duplicates:** The fetched memory blocks are then passed to the `analyze_duplicates_with_llm` function, which is responsible for generating consolidation suggestions. If the LLM is unavailable or fails, a fallback similarity analysis is performed *only* to identify groups, but no LLM-generated suggestions will be created.
3.  **Store Suggestions:** Only consolidation suggestions generated by the LLM are stored in the `consolidation_suggestions` table in the database via the `store_consolidation_suggestions` function. This function also includes logic to prevent the creation of duplicate pending suggestions.

## 3. Duplicate Analysis Methods

### 3.1. LLM-based Analysis (Primary Method - Mandatory for Suggestions)

The `analyze_duplicates_with_llm` function uses the Google Gemini API for advanced semantic analysis and **is mandatory for generating consolidation suggestions.**

*   **Model Used:** Configurable via `LLM_MODEL_NAME` environment variable (e.g., `gemini-2.5-flash-preview-05-20`).
*   **Prompt Structure:** A detailed prompt is constructed to guide the LLM. It instructs the model to act as an AI assistant, identify semantically similar or duplicate memory blocks, group them, and then generate a consolidated version of the `content`, `lessons_learned`, and `keywords` for each group.
    *   **Goal of Consolidation:** The prompt specifically aims to ensure that the consolidated memory blocks exhibit **increased quality** and **increased density of information**, leading to a **reduced overall size** compared to the sum of original blocks. The length of the consolidated block should not increase at all costs, to prevent indefinite growth and maintain the quality and relevance of memory. **Crucially, the generated `suggested_content` and `suggested_lessons_learned` are programmatically trimmed to not exceed the maximum length of any individual original memory block in the group, ensuring conciseness.**
    *   **Input Data to LLM:** Each memory block is provided to the LLM as a JSON object containing its `id`, `content`, `lessons_learned`, and `keywords`.
    *   **Output Format:** The LLM is instructed to return its response in a strict JSON format:
        ```json
        {
          "groups": [
            {
              "group_id": "string",
              "memory_ids": ["string", ...],
              "suggested_content": "string",
              "suggested_lessons_learned": "string",
              "suggested_keywords": ["string", ...]
            },
            // ... more groups
          ]
        }
        ```
*   **Configuration:**
    *   `LLM_API_KEY`: **Required** environment variable for Gemini API access. If not set, LLM-based suggestion generation will not occur.
    *   `response_mime_type`: Set to `application/json` to ensure structured output.
    *   `temperature`: Set to `0.3` to encourage more deterministic and less creative responses, suitable for this task.

### 3.2. Fallback Similarity Analysis (For Grouping Only)

The `analyze_duplicates_with_fallback` function is used when the LLM-based analysis is not possible or fails. **Crucially, this method only identifies groups of similar memory blocks and does NOT generate consolidated suggestions.** If this method is used, no new consolidation suggestions will be created in the database, as LLM-generated suggestions are mandatory for quality and density.

*   **Method:** It employs a traditional text similarity approach:
    1.  **TF-IDF Vectorization:** Combines the `content` and `lessons_learned` of memory blocks and converts them into TF-IDF (Term Frequency-Inverse Document Frequency) vectors.
    2.  **Cosine Similarity:** Calculates the cosine similarity between these TF-IDF vectors to determine how similar two memory blocks are.
*   **Thresholding:** Memory blocks with a cosine similarity score equal to or above `FALLBACK_SIMILARITY_THRESHOLD` (default: `0.4` for testing, typically `0.85` for production) are grouped as duplicates.
*   **No Consolidation Generation:** This method does not generate `suggested_content`, `suggested_lessons_learned`, or `suggested_keywords`. Its output is limited to `group_id` and `memory_ids`.

## 4. Storing Consolidation Suggestions

The `store_consolidation_suggestions` function handles the persistence of the generated suggestions:

*   It iterates through the `duplicate_groups` identified by the analysis step.
*   **Only groups that include LLM-generated `suggested_content` and `suggested_lessons_learned` will be considered for storage.** If these fields are missing (indicating fallback analysis was used), the group will be skipped.
*   Before creating a new suggestion, it checks for existing pending suggestions that have overlapping `original_memory_ids` to prevent redundant suggestions.
*   New suggestions are created in the `consolidation_suggestions` table with a `status` of "pending", awaiting user review and validation.
*   UUIDs for `group_id` and `original_memory_ids` are ensured to be in string format for proper JSONB serialization in the database, aligning with the `schemas.py` definition.
